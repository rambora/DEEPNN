{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 10 # 1st layer number of features\n",
    "n_hidden_2 = 10 # 2nd layer number of features\n",
    "n_input = 10 # Number of feature\n",
    "n_classes = 2 # Number of classes to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean the graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with sigmoid activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define the cost/losss\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=pred)\n",
    "    #xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)\n",
    "    cost = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(cost)\n",
    "\n",
    "# Evaluation\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras kind of MLP construct in TF (using layers.dense)\n",
    "# Define the network\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name='outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN\n",
    "# This specifies the weights for either fully connected or convolutional layers \n",
    "# of the network. They are initialized randomly using a truncated normal distribution\n",
    "# with a standard deviation of 0.1. This sort of initialization with a random normal\n",
    "# distribution that is truncated at the tails is pretty common and generally produces\n",
    "# good results\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    # initial = tf.random_normal()\n",
    "    return tf.Variable(initial)\n",
    "# This defines the bias elements in either fully connected or a convolutional layer.\n",
    "# These are all initialized with the constant value of 0.1\n",
    "def bias_variable(shape):\n",
    "    initial = tf.Constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "# This specifies the convolution we will typically use. A full convolution (no skips)\n",
    "# withh an output the same size as the input\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strindes=[1,1,1,1], padding='SAME')\n",
    "# This sets the max pool to half the size across the height/width dimensions, and in \n",
    "# total a quarter the size of the feature map\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "# This is the actual layer we will use. Linear convolution as defined in conv2d, with a bias,\n",
    "# followed by te ReLU nonlinearity\n",
    "def conv_layer(input, shape):\n",
    "    W = weight_variable(shape)\n",
    "    b = bias_variable(shape)\n",
    "    return tn.nn.relu(conv2d(input, W) + b)\n",
    "# A standard full layer with a bias. Notice that here we didn't add the ReLU. This allows\n",
    "# us to use the same layer for the final output, where we don't need the non-linar part.\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = weight_variable([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "conv1 = conv_layer(x_image, shape=[5,5,1,32])\n",
    "conv1_pool = max_pool_2x2(conv1)\n",
    "\n",
    "conv2 = conv_lauer(conv1_pool, shape=[5,5,32,64])\n",
    "conv2_pool = max_pool_2x2(conv2)\n",
    "\n",
    "conv2_flat = tf.reshape(conv2_pool, [-1, 7*7*64])\n",
    "full_1 = tf.nn.relu(full_layer(conv2_flat, 1024))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "full_drop = tf.nn.dropout(full_1, keep_prob= kepp_prob)\n",
    "\n",
    "y_conv = full_layer(full_drop, 10)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN cells\n",
    "# static unrolling through time\n",
    "x0/x1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [x0,x1],dtype=tf.float32)\n",
    "\n",
    "# dynamic unrolling through time\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "output_seqs, states = tf.nn.dynamic_rnn(basic_cell, x, dtype=tf.float32)\n",
    "\n",
    "# multi-layer cells (statcking multiple RNNs)\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRnnCell(basic_cell)\n",
    "output_seqs, states = tf.nn.dynamic_rnn(multi_layer_cell, x, dtype=tf.float32)\n",
    "\n",
    "# sequence classifier\n",
    "prediction = tf.layers.dense(outputs[:, -1], 1, activation=tf.sigmoid)\n",
    "softmax = tf.nn.softmax_cross_entropy-with_logits(lotits=final_output, labels=y)\n",
    "\n",
    "# applying dropout\n",
    "cells = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) for layer in range(n_layers)]\n",
    "cells_drop = [tf.contrib.rnn.DropoutWrapper(cell, inout_keep_prob=keep_prob) for cell in cells]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRnnCell(cell_drop)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "\n",
    "# Basic LSTM cell\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)\n",
    "# peephole connections\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, use_peepholes=True)\n",
    "\n",
    "# GRU cell\n",
    "gru_cell = tf.contrib.rnn.GRUCell(num_units=n_neurons)\n",
    "\n",
    "# Embeddings\n",
    "init_embeds = tf.random_uniform([vocabulary_size, embedding_size], -1.0,1.0)\n",
    "embeddings = tf.Variable(init_embeds)\n",
    "train_inputs = tf.placeholder(tf.float32, shape=[None])\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# pre-trained embeddings\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, GLOVE_SIZE])\n",
    "if PRE_TRAINED:\n",
    "    embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, GLOVE_SIZE]), trainable=True)\n",
    "    # if using pretrained embeddings, assign them to the embeddings variable\n",
    "    embedding_init = embeddings.assign(embedding_placeholder)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "else:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_dimension],-1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "    \n",
    "# Bidirectional\n",
    "with tf.name_scope('biGRU'):\n",
    "    with tf.variable_scope('forward'):\n",
    "        gru_fw_cell = tf.contrib.rnn.GRUCell(hidden_layer_size)\n",
    "        gru_fw_cell = tf.contrib.rnn.DropoutWrapper(gru_fw_cell)\n",
    "    with tf.variable_scope('backward'):\n",
    "        gru_bw_cell = tf.contrib.rnn.GRUCell(hidden_layer_size)\n",
    "        gru_bw_cell = tf.contrib.rnn.DropoutWrapper(gru_bw_cell)\n",
    "        \n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw=gru_fw_cell,\n",
    "                                                     cell_bw=gru_bw_cell, input=embed, sequence_length=_seqlens,\n",
    "                                                     dtype=tf.float32, scope='BiGRU')\n",
    "    states = tf.concat(values=states, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
