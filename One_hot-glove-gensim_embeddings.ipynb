{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Demonstration of usage of word embeddings (derived from keras one-hot, glove and gensim) in keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = ['this is the first sentence for word2vec','this is the second sentence','yet another sentence','one more sentence', \n",
    "            'and the final sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'], ['this', 'is', 'the', 'second', 'sentence'], ['yet', 'another', 'sentence'], ['one', 'more', 'sentence'], ['and', 'the', 'final', 'sentence']]\n"
     ]
    }
   ],
   "source": [
    "# Define the training data\n",
    "tokens = [i.split() for i in sentence]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'this', 'is', 'the', 'second', 'sentence', 'yet', 'another', 'sentence', 'one', 'more', 'sentence', 'and', 'the', 'final', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "# reducing the lists of lists into a flat list\n",
    "from functools import reduce\n",
    "flat_list = reduce(lambda x,y: x+y,tokens)\n",
    "print(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n",
      "['and', 'word2vec', 'for', 'sentence', 'this', 'is', 'one', 'second', 'another', 'the', 'first', 'yet', 'final', 'more']\n",
      "[  1.09108200e-03  -1.50774524e-03  -3.20697599e-03  -9.94841801e-04\n",
      "   3.15078214e-04   4.37214086e-03  -3.69665888e-03  -3.33983405e-03\n",
      "  -4.54258965e-03   9.99633805e-04  -4.23612585e-03  -3.83223081e-03\n",
      "   4.32860339e-03  -3.25149111e-03   2.95695267e-03  -3.71881900e-03\n",
      "  -1.00519822e-03   4.84643737e-03  -3.53238801e-03   1.22420708e-04\n",
      "  -4.58742678e-03   3.71732470e-03  -4.15829709e-03  -3.93305486e-03\n",
      "  -8.70488118e-04  -4.59041959e-03   3.76840495e-03  -3.98694444e-03\n",
      "   1.56842021e-03   4.36723279e-03   1.75691827e-03   1.42543553e-03\n",
      "  -3.02267703e-03   1.56308757e-03   1.13664242e-03  -1.06631417e-03\n",
      "   4.95949062e-03   1.51077483e-03  -3.55202728e-03  -1.73709460e-03\n",
      "  -4.47266269e-03  -3.67703475e-03   3.55967181e-03  -7.38290721e-04\n",
      "   7.48101273e-04   2.56760861e-03   8.07312434e-04  -2.70612515e-03\n",
      "  -4.23209416e-03   3.43392487e-03   9.29186004e-04   4.30594292e-03\n",
      "  -1.68683392e-03   2.32448475e-03  -3.98656679e-03   4.22142725e-03\n",
      "   3.05290613e-03   2.75243446e-03   4.32301033e-03  -3.94099904e-03\n",
      "  -2.20075296e-03  -2.56885798e-03  -4.12407611e-03  -1.56984746e-03\n",
      "  -3.00061726e-03  -3.19348928e-03   2.11892696e-03  -4.91847843e-03\n",
      "  -2.95917527e-03  -2.72088544e-03  -3.68361338e-03   4.82129958e-03\n",
      "  -3.23147955e-03   7.92523380e-04  -1.13938085e-03   4.13301494e-03\n",
      "  -2.28527421e-03   1.18826900e-03   4.92700469e-03   1.03034917e-03\n",
      "  -2.92705768e-03   2.56462279e-03   3.56607517e-04   3.56026203e-03\n",
      "   7.45267535e-05  -4.00806777e-04  -3.83335561e-03  -1.51982182e-03\n",
      "  -1.27809716e-03  -3.43685434e-03  -3.12654581e-03   2.05130153e-03\n",
      "  -4.02728189e-03  -3.57793015e-03  -1.95463211e-03  -1.00961013e-03\n",
      "   2.47778278e-03  -1.26721070e-03  -3.62945604e-03  -1.76866830e-03]\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# duplicate words will be taken care automatically\n",
    "# by default, every single word will be represented by a 100 dimensional vector in the embedded space\n",
    "model = Word2Vec(tokens, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "# save model\n",
    "#model.save('model.bin')\n",
    "# load model\n",
    "#new_model = Word2Vec.load('model.bin')\n",
    "#print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIl/8PQKolbHMnUH0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6de16aa350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot word vectors using PCA\n",
    "from matplotlib import pyplot\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X = model[model.wv.vocab]\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot\n",
    "pyplot.scatter(result[:,0],result[:,1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i,0], result[i,1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs1 = ['Well done!; Good work; Great effort; nice work; Excellent; Weak; Poor effort!; not good; poor work; Could have done better.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Well done!', ' Good work', ' Great effort', ' nice work', ' Excellent', ' Weak', ' Poor effort!', ' not good', ' poor work', ' Could have done better.']]\n"
     ]
    }
   ],
   "source": [
    "docs = [i.split(';') for i in docs1]\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Well done!', ' Good work', ' Great effort', ' nice work', ' Excellent', ' Weak', ' Poor effort!', ' not good', ' poor work', ' Could have done better.']\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "docs_flat = reduce(lambda x,y: x+y,docs)\n",
    "print(docs_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Labelling/scoring the sentiment\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "print(labels)\n",
    "# In reality, this kind of labelling comes either from supervised learning/WordClouds/LDA/LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing using keras one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25, 48], [18, 22], [39, 3], [17, 22], [28], [37], [3, 3], [40, 34], [3, 22], [40, 19, 48, 4]]\n"
     ]
    }
   ],
   "source": [
    "# integer encoding the documents using keras one_hot encoding \n",
    "# This is not the same hot_encoding rather indexing based on the location\n",
    "vocab_size =50\n",
    "encoded_docs = [one_hot(d,vocab_size) for d in docs_flat]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the sequences have different lengths and the integers indicates the location of that specific word in the vocabulary space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25 48  0  0]\n",
      " [18 22  0  0]\n",
      " [39  3  0  0]\n",
      " [17 22  0  0]\n",
      " [28  0  0  0]\n",
      " [37  0  0  0]\n",
      " [ 3  3  0  0]\n",
      " [40 34  0  0]\n",
      " [ 3 22  0  0]\n",
      " [40 19 48  4]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 4, 32)             1600      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,729\n",
      "Trainable params: 1,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# modeling\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 32, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here the output of the embedding layer is 4x8 matrix (4=input_length, 8-output of the embedded vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.9999976158\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0) # padded_docs - one-hot encoded/indexed\n",
    "#evaluate the model\n",
    "loss,accuracy = model.evaluate(padded_docs, labels,verbose=0)\n",
    "print('Accuracy: {}'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# vectorization using Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Well done!', ' Good work', ' Great effort', ' nice work', ' Excellent', ' Weak', ' Poor effort!', ' not goot', ' poor work', ' Could have done better.']\n"
     ]
    }
   ],
   "source": [
    "print(docs_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'poor': 4, 'great': 7, 'good': 6, 'weak': 10, 'could': 13, 'work': 1, 'well': 5, 'better': 15, 'goot': 12, 'done': 2, 'have': 14, 'excellent': 9, 'not': 11, 'effort': 3, 'nice': 8}\n",
      "16\n",
      "[[5, 2], [6, 1], [7, 3], [8, 1], [9], [10], [4, 3], [11, 12], [4, 1], [13, 14, 2, 15]]\n",
      "[[ 5  2  0  0]\n",
      " [ 6  1  0  0]\n",
      " [ 7  3  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 4  3  0  0]\n",
      " [11 12  0  0]\n",
      " [ 4  1  0  0]\n",
      " [13 14  2 15]]\n"
     ]
    }
   ],
   "source": [
    "# prepare tokenizer - unlike in keras one-hot, here we are not predefining the vocab_size rather getting it from the \n",
    "# vocabulary itself\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs_flat)\n",
    "# creates the word_to_index dictionary \n",
    "vocab_size2 = len(t.word_index) + 1\n",
    "print(t.word_index)\n",
    "print(vocab_size2)\n",
    "# integer encode the documents\n",
    "# creates the integer vectors based on the words in the sentences\n",
    "encoded_docs2 = t.texts_to_sequences(docs_flat)\n",
    "print(encoded_docs2)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs2, maxlen=max_length, padding = 'post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using pre-trained Glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "\n",
    "embeddings_index = dict()\n",
    "with open('/home/ramscrux7757/SPARK/glove/glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print('Loaded {} word vectors'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\n",
       "       -0.20558   , -0.41846001, -0.58437002, -0.77354997, -0.87866002,\n",
       "       -0.37858   , -0.18516   , -0.12800001, -0.20584001, -0.22925   ,\n",
       "       -0.42598999,  0.3725    ,  0.26076999, -1.07019997,  0.62915999,\n",
       "       -0.091469  ,  0.70348001, -0.4973    , -0.77691001,  0.66044998,\n",
       "        0.09465   , -0.44893   ,  0.018917  ,  0.33146   , -0.35021999,\n",
       "       -0.35789001,  0.030313  ,  0.22253001, -0.23236001, -0.19719   ,\n",
       "       -0.0053125 , -0.25848001,  0.58081001, -0.10705   , -0.17845   ,\n",
       "       -0.16205999,  0.087086  ,  0.63028997, -0.76648998,  0.51618999,\n",
       "        0.14072999,  1.01900005, -0.43136001,  0.46138   , -0.43584999,\n",
       "       -0.47567999,  0.19226   ,  0.36065   ,  0.78987002,  0.088945  ,\n",
       "       -2.78139997, -0.15366   ,  0.01015   ,  1.17980003,  0.15167999,\n",
       "       -0.050112  ,  1.26259995, -0.77526999,  0.36030999,  0.95761001,\n",
       "       -0.11385   ,  0.28035   , -0.02591   ,  0.31246001, -0.15424   ,\n",
       "        0.37779999, -0.13598999,  0.29460001, -0.31579   ,  0.42943001,\n",
       "        0.086969  ,  0.019169  , -0.27241999, -0.31696001,  0.37327   ,\n",
       "        0.61997002,  0.13889   ,  0.17188001,  0.30362999, -1.27760005,\n",
       "        0.044423  , -0.52736002, -0.88536   , -0.19428   , -0.61947   ,\n",
       "       -0.10146   , -0.26301   , -0.061707  ,  0.36627001, -0.95222998,\n",
       "       -0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "('poor', 4)\n",
      "('great', 7)\n",
      "('good', 6)\n",
      "('weak', 10)\n",
      "('could', 13)\n",
      "('work', 1)\n",
      "('well', 5)\n",
      "('better', 15)\n",
      "('goot', 12)\n",
      "('done', 2)\n",
      "('have', 14)\n",
      "('excellent', 9)\n",
      "('not', 11)\n",
      "('effort', 3)\n",
      "('nice', 8)\n",
      "(16, 100)\n",
      "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [-0.11619     0.45447001 -0.69216001 ..., -0.54737002  0.48822001  0.32246   ]\n",
      " [-0.2978      0.31147    -0.14937    ..., -0.22709    -0.029261    0.4585    ]\n",
      " ..., \n",
      " [ 0.05869     0.40272999  0.38633999 ..., -0.35973999  0.43718001  0.10121   ]\n",
      " [ 0.15711001  0.65605998  0.0021149  ..., -0.60614997  0.71004999\n",
      "   0.41468999]\n",
      " [-0.047543    0.51914001  0.34283999 ..., -0.26859     0.48664999  0.55609   ]]\n"
     ]
    }
   ],
   "source": [
    "# create a weeight matrix for words in training docs\n",
    "# the embedding matrix is a (doc_size x embedded space) dimension matrix\n",
    "# every doc is a vector of 100 elements (in this case)\n",
    "embedding_matrix = np.zeros((vocab_size2, 100))\n",
    "print(embedding_matrix)\n",
    "for word, i in t.word_index.items():\n",
    "    print(word,i)\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if  embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 4, 100)            1600      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 2,001\n",
      "Trainable params: 401\n",
      "Non-trainable params: 1,600\n",
      "_________________________________________________________________\n",
      "Accuracy 100.0:\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size2, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "#compile the model\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "# summarize the model\n",
    "model.summary()\n",
    "# fit  the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss,accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy {}:'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating word-embeddings using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'poor': 4, 'great': 7, 'good': 6, 'weak': 10, 'could': 13, 'work': 1, 'well': 5, 'better': 15, 'goot': 12, 'done': 2, 'have': 14, 'excellent': 9, 'not': 11, 'effort': 3, 'nice': 8}\n"
     ]
    }
   ],
   "source": [
    "# following the vectorization from Tokenizer\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poor', 'great', 'good', 'weak', 'could', 'work', 'well', 'better', 'goot', 'done', 'have', 'excellent', 'not', 'effort', 'nice']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "docs_lst2 = t.word_index.keys()\n",
    "print(docs_lst2)\n",
    "print(len(docs_lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=15, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Generating embedding model using gensim\n",
    "\n",
    "# train model \n",
    "ebd_model = Word2Vec([docs_lst2], min_count = 1)\n",
    "# summrize the moaded model\n",
    "print(ebd_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -4.81037796e-03   2.09023221e-03   2.35071778e-03   3.80843412e-04\n",
      "   3.75166885e-03   4.53934772e-03   4.93540382e-03  -9.96194780e-04\n",
      "   1.39838620e-03  -2.16753615e-04  -5.28400531e-04   2.95642950e-03\n",
      "  -2.63809762e-03   2.56817159e-03  -3.79028730e-03  -4.49219486e-03\n",
      "   2.77458853e-03   3.47033492e-03  -1.86004990e-03  -8.05604504e-04\n",
      "   1.81936088e-03  -3.94339254e-03  -1.48025167e-03  -4.25600359e-04\n",
      "  -4.63156641e-04  -2.23293738e-03   4.10455698e-03   2.98301084e-03\n",
      "   4.90180869e-03   3.21363244e-04  -3.66398925e-03  -2.08852789e-03\n",
      "   3.35919298e-03  -4.69565894e-05  -3.90784303e-03  -1.36258849e-03\n",
      "  -2.37535173e-03   3.61863407e-03   1.23370707e-03   2.48365849e-03\n",
      "   1.72860827e-03  -4.00261348e-03   2.78630963e-04   2.31878436e-03\n",
      "  -4.96223290e-03  -2.78676418e-03  -1.46768332e-04   3.92311765e-03\n",
      "   1.10038812e-03  -3.97884799e-03   7.20909622e-04  -2.49280711e-04\n",
      "   1.52486830e-03   6.63264014e-04  -4.83806152e-03   4.91132913e-03\n",
      "   2.53371196e-03  -2.23401678e-03  -1.34384245e-04  -2.34644092e-03\n",
      "   2.87103234e-03  -2.76275282e-03  -8.80814914e-04  -4.16154740e-03\n",
      "   2.89162109e-03   1.05749990e-03  -4.65451553e-03  -1.40682780e-04\n",
      "   8.86975264e-04   2.26964662e-03   3.45068844e-03  -2.66028964e-03\n",
      "   1.66836416e-03   4.93951282e-03  -2.37547513e-03  -1.46794261e-03\n",
      "   1.89330324e-03  -3.03404313e-03   2.78600166e-03  -1.67390204e-03\n",
      "  -3.41392960e-03  -4.75505693e-03   7.87753728e-04  -1.38122006e-03\n",
      "   1.58585713e-03  -2.76982994e-03   4.18053335e-03  -4.38915240e-03\n",
      "  -3.48702283e-03  -5.84913243e-04   3.18408641e-03  -3.69918300e-04\n",
      "  -4.87939804e-04  -3.61927180e-03   1.37804847e-04   3.84360412e-03\n",
      "  -1.96577562e-03   3.84394708e-03   1.44666876e-03   2.23670457e-03]\n"
     ]
    }
   ],
   "source": [
    "print(ebd_model['done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('poor', 4),\n",
       " ('great', 7),\n",
       " ('good', 6),\n",
       " ('weak', 10),\n",
       " ('could', 13),\n",
       " ('work', 1),\n",
       " ('well', 5),\n",
       " ('better', 15),\n",
       " ('goot', 12),\n",
       " ('done', 2),\n",
       " ('have', 14),\n",
       " ('excellent', 9),\n",
       " ('not', 11),\n",
       " ('effort', 3),\n",
       " ('nice', 8)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 100)\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [ -3.16592705e-05   7.61602831e-04   3.24354065e-03 ...,  -2.95457011e-03\n",
      "    1.06048153e-03   2.95024808e-03]\n",
      " [ -4.81037796e-03   2.09023221e-03   2.35071778e-03 ...,   3.84394708e-03\n",
      "    1.44666876e-03   2.23670457e-03]\n",
      " ..., \n",
      " [  2.36723083e-03   4.02091508e-04   9.88070969e-04 ...,  -1.36979274e-03\n",
      "   -1.13945687e-03   1.90868753e-03]\n",
      " [ -5.15873719e-04  -3.15627130e-03   2.49778945e-03 ...,  -3.30333016e-03\n",
      "   -1.95337366e-03  -3.08220508e-03]\n",
      " [  3.36224190e-03  -2.24425760e-03   2.30989812e-04 ...,  -1.01240037e-03\n",
      "    6.21207757e-04  -2.73332582e-03]]\n"
     ]
    }
   ],
   "source": [
    "# create a weeight matrix for words in training docs\n",
    "vocab_size3 = len(set(docs_lst2)) + 1\n",
    "embedding_matrix2 = np.zeros((vocab_size3, 100))\n",
    "\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = ebd_model[word]\n",
    "    #embedding_vector = embeddings_index.get(word)\n",
    "    if  embedding_vector is not None:\n",
    "        embedding_matrix2[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix2.shape)\n",
    "print(embedding_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 4, 100)            1600      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 2,001\n",
      "Trainable params: 401\n",
      "Non-trainable params: 1,600\n",
      "_________________________________________________________________\n",
      "Accuracy 100.0:\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "e2 = Embedding(vocab_size3, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e2)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "#compile the model\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "# summarize the model\n",
    "model.summary()\n",
    "# fit  the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss,accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy {}:'.format(accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
