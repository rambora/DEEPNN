{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. L1/L2 Regularization\n",
    "\n",
    "from keras import regularizers\n",
    "model.add(Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01)\n",
    "\n",
    "# Here, 0.01 is the lamda value (regularization parameter, which could be a hyper parameter aswell \n",
    "# and can optimize using grid-search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Dropout\n",
    "\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "model = Sequential([\n",
    " Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu'),\n",
    " Dropout(0.25),\n",
    "\n",
    "Dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='softmax'),])\n",
    "\n",
    "# Dropout is like an ensemble model (randomly dropsout the 25% of the neurons for every epoch)\n",
    "# being ensemble in nature, at times, this produces better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Data Augmentation (in keras, we use ImageDataGenerator for this)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(horizontal flip=True)\n",
    "datagen.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. Early Stopping\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "EarlyStopping(monitor='val_err', patience=5)\n",
    "\n",
    "# monitor - which metric to monitor\n",
    "# patience - number of epochs with no further improvement after which the training will be stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. Batch Normalization\n",
    "# generally, one uses this between the linear and nonlinear layers (it normalizes the input to your non-linear function)\n",
    "\n",
    "# import BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# instantiate model\n",
    "model = Sequential()\n",
    "\n",
    "# we can think of this chunk as the input layer\n",
    "model.add(Dense(64, input_dim=14, init='uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(Convolution2D(64, 3, 3, use_bias=False))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "# we can think of this chunk as the hidden layer    \n",
    "model.add(Dense(64, init='uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# we can think of this chunk as the output layer\n",
    "model.add(Dense(2, init='uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# setting up the optimization of our weights \n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "\n",
    "# running the fitting\n",
    "model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)\n",
    "\n",
    "# For CNN's\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\",input_shape=inputShape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\",input_shape=inputShape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch Normalization in TensorFlow\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden_layers, name='hidden1')\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden_layers, name='hidden2')\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. Gradient Clipping\n",
    "\n",
    "rsmprop = RMSprop(clipvalue=0.5)\n",
    "#or\n",
    "rmsprop = RMSprop(clipnorm=1.0)\n",
    "# all parameter gradients will be clipped to\n",
    "# a maximum value of 0.5 and\n",
    "# a minimum value of -0.5.\n",
    "model.compile(loss='mse', optimizer=rsmprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient Clipping in TensorFlow\n",
    "\n",
    "threshold = 1.0\n",
    "optimizer = tf.train.GradinetDescenetOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "#it will compute the gradients and clip them between -1.0 and 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizers in NN\n",
    "\n",
    "1. momentum optimizer\n",
    "2. Nesterov Accelerated Gradient\n",
    "3. AdaGrad\n",
    "4. RMSProp\n",
    "5. Adam Optimization\n",
    "6. SGD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
