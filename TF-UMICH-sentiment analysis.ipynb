{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data source: (https://www.kaggle.com/c/si650winter11)\n",
    "train_data = pd.read_csv('/home/ramscrux7757/SPARK/SENT_ANALS/UMICH/Train.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>The Da Vinci Code book is just awesome.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>that's not even an exaggeration ) and at midni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1            The Da Vinci Code book is just awesome.\n",
       "0  1  this was the first clive cussler i've ever rea...\n",
       "1  1                   i liked the Da Vinci Code a lot.\n",
       "2  1                   i liked the Da Vinci Code a lot.\n",
       "3  1  I liked the Da Vinci Code but it ultimatly did...\n",
       "4  1  that's not even an exaggeration ) and at midni..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>that's not even an exaggeration ) and at midni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                             Review\n",
       "0      1  this was the first clive cussler i've ever rea...\n",
       "1      1                   i liked the Da Vinci Code a lot.\n",
       "2      1                   i liked the Da Vinci Code a lot.\n",
       "3      1  I liked the Da Vinci Code but it ultimatly did...\n",
       "4      1  that's not even an exaggeration ) and at midni..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns = ['Class','Review']\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>harvard is dumb, i mean they really have to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm loving Shanghai &gt; &gt; &gt; ^ _ ^.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>harvard is for dumb people.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As i stepped out of my beautiful Toyota, i hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bodies being dismembered, blown apart, and mut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review\n",
       "0  harvard is dumb, i mean they really have to be...\n",
       "1                   I'm loving Shanghai > > > ^ _ ^.\n",
       "2                        harvard is for dumb people.\n",
       "3  As i stepped out of my beautiful Toyota, i hea...\n",
       "4  Bodies being dismembered, blown apart, and mut..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('/home/ramscrux7757/SPARK/SENT_ANALS/UMICH/Test.txt', sep='\\t')\n",
    "test_data.columns = ['Review']\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffling the data as currently its being arranged by class\n",
    "from sklearn.utils import shuffle\n",
    "train_data = shuffle(train_data)\n",
    "test_data = shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separating the lables and Review data\n",
    "train_labels = train_data['Class'].values\n",
    "train_rev_data = train_data['Review'].values\n",
    "test_rev_data = test_data['Review'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6917,)\n",
      "(28936,)\n",
      "(35853,)\n"
     ]
    }
   ],
   "source": [
    "# concatenating train and test reviews to generate Vocabulary\n",
    "print(train_data['Review'].shape)\n",
    "print(test_data['Review'].shape)\n",
    "train_rev = train_data['Review']\n",
    "test_rev = test_data['Review']\n",
    "train_test_vocab = pd.concat([train_rev, test_rev], axis=0)\n",
    "print(train_test_vocab.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-process the data (could be cleaned in a better way, but the following is just for the demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following creates the word_to_index mappings\n",
    "def tokenizer(data_doc):\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(data_doc)\n",
    "    # creates the word_to_index dictionary \n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #t.word_index = {k:v for k, v in t.word_index.items() if k not in stop_words}\n",
    "    #t.word_index = {k:v for k, v in t.word_index.items() if len(k) > 2}\n",
    "    #print(t.word_index)\n",
    "\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    print('vocab_size: {}'.format(vocab_size))\n",
    "    #return vocab_size\n",
    "\n",
    "    # integer encode the documents\n",
    "    # creates the integer vectors based on the words in the sentences\n",
    "    encoded_docs = t.texts_to_sequences(data_doc)\n",
    "    return vocab_size, encoded_docs\n",
    "    #print(encoded_docs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 10722\n",
      "6917\n",
      "28936\n"
     ]
    }
   ],
   "source": [
    "vocab_size, train_test_enc_docs = tokenizer(train_test_vocab)\n",
    "train_enc_docs = train_test_enc_docs[:6917]\n",
    "test_enc_docs = train_test_enc_docs[6917:]\n",
    "print(len(train_enc_docs))\n",
    "print(len(test_enc_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# identifying the max length of the sequences followed by the padding of the sequences\n",
    "def seq_max_length(enc_doc, train_test_enc_docs):\n",
    "    max_length = max([len(doc) for doc in train_test_enc_docs])\n",
    "    #print('train_max_length: {}'.format(train_max_length))\n",
    "    padded_docs = pad_sequences(enc_doc, maxlen=max_length, padding = 'post')\n",
    "    print(padded_docs.shape)\n",
    "    return padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6917, 1891)\n",
      "(28936, 1891)\n"
     ]
    }
   ],
   "source": [
    "train_padded_docs = seq_max_length(train_enc_docs, train_test_enc_docs)\n",
    "test_padded_docs = seq_max_length(test_enc_docs, train_test_enc_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400, 1891)\n"
     ]
    }
   ],
   "source": [
    "# train-test splittings\n",
    "X_train = train_padded_docs[:6400]\n",
    "Y_train = train_labels[:6400]\n",
    "X_test = train_padded_docs[6400:]\n",
    "Y_test = train_labels[6400:]\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1891)\n",
      "(1000,)\n",
      "(400, 1891)\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "# subsampling the data for simplicity\n",
    "X_train = X_train[:1000]\n",
    "Y_train = Y_train[:1000]\n",
    "X_test = X_test[:400]\n",
    "Y_test = Y_test[:400]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining RNN\n",
    "hidden_layer_size = 10 # 512 # how many nodes LSTM cells will have\n",
    "number_of_layers = 1 # how many RNN layers the network will use\n",
    "batch_size = 100 #100 # how many reviews we feed at onces\n",
    "learning_rate = 0.001 # learning rate\n",
    "number_of_words = vocab_size + 1\n",
    "#number_of_words = len(vocab_to_int) + 1 #how many unique words do we have in vocab (+1  is used for 0 - padding)\n",
    "dropout_rate = 0.8 \n",
    "embed_size = 300 #how long our word embedings will be\n",
    "epochs = 2 # how many epochs do we use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean the existing graph if any\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the place holders\n",
    "inputs = tf.placeholder(tf.int32, [None, None], name='inputs') # batch-size/n_inputs and time_steps/n_steps\n",
    "targets = tf.placeholder(tf.int32, [None, None], name='targets') # batch_size and num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define embedding layer\n",
    "word_embedings = tf.Variable(tf.random_uniform((number_of_words, embed_size), -1, 1)) # number_of_words = vocab_size + 1\n",
    "embed = tf.nn.embedding_lookup(word_embedings, inputs)\n",
    "# in Keras\n",
    "# model.add(Embedding(vocab_size2, 100, weights=[embedding_matrix], input_length=4, trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the network (hidden layer and Dynamic RNN)\n",
    "\n",
    "def LSTM_dynamic_RNN(number_of_layers, hidden_layer_size, dropout_rate, batch_size, embed):\n",
    "    \n",
    "    hidden_layer = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size)\n",
    "    hidden_layer = tf.contrib.rnn.DropoutWrapper(hidden_layer, dropout_rate)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([hidden_layer]*number_of_layers)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # feeding the basic LSTM cell into the dynamic RNN\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embed, initial_state = init_state)\n",
    "    # here the 'cell' could also be the 'hidden_layer' or simply the 'BasicRNNCell'\n",
    "    # initial_state could be the 'inputs'\n",
    "    # for static, replace 'tf.nn.dynamic_rnn' with 'tf.contrib.rnn.static_rnn'\n",
    "\n",
    "    # Get the prediction for each row / sequence classifier\n",
    "    prediction = tf.layers.dense(outputs[:, -1], 1, activation=tf.sigmoid)\n",
    "    return prediction\n",
    "    #return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = LSTM_dynamic_RNN(number_of_layers, hidden_layer_size, dropout_rate, batch_size, embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Loss and Optimizer\n",
    "cost = tf.losses.mean_squared_error(targets, prediction)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the accuracy\n",
    "currect_pred = tf.equal(tf.cast(tf.round(prediction), tf.int32), targets)\n",
    "accuracy = tf.reduce_mean(tf.cast(currect_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python2.7/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1/2', ' | Current loss: nan', ' | Training accuracy: 57.1000')\n",
      "('Epoch: 2/2', ' | Current loss: nan', ' | Training accuracy: 57.5000')\n",
      "Optimization Finished!\n",
      "\n",
      "Validation Step\n",
      "Test accuracy is 55.5000%\n",
      "Predictions are complete !!!\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph (training, validation and prediction)\n",
    "# here, training, validation and predictions are put within one cell which is inconvinient.\n",
    "# instead, put\n",
    "# session = tf.Session()\n",
    "# session.run(tf.global_variables_initializer())\n",
    "# in separate cells and separate the training, validation and predictions \n",
    "# precisely, we are not using 'with...'\n",
    "\n",
    "display_step = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        training_acc = []\n",
    "        training_cost = []\n",
    "        #avg_cost = 0.\n",
    "        total_batch = int(len(X_train)/batch_size)\n",
    "\n",
    "        X_batches = np.array_split(X_train, total_batch)\n",
    "        Y_batches = np.array_split(Y_train, total_batch)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = X_batches[i], Y_batches[i].reshape(-1,1)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            a, c, _ = sess.run([accuracy, cost, optimizer], feed_dict={inputs: batch_x, targets: batch_y})\n",
    "        \n",
    "            #----------------------------------------------------------\n",
    "            # in Keras\n",
    "            # model.fit(x_train,y_train,epochs=5, batch_size=32)\n",
    "            #-----------------------------------------------------------\n",
    "            # Compute average loss\n",
    "            training_acc.append(a)\n",
    "            #training_cost.append(c)\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            \n",
    "           print('Epoch: {}/{}'.format(epoch+1, epochs), ' | Current loss: {}'.format(np.mean(training_cost)),\n",
    "            ' | Training accuracy: {:.4f}'.format(np.mean(training_acc)*100))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print\n",
    "    #------------------------------------------------------------\n",
    "    print('Validation Step')\n",
    "    \n",
    "    # validation / Test set\n",
    "\n",
    "    val_acc = []\n",
    "\n",
    "    val_batch = int(len(X_test)/batch_size)\n",
    "    X_val_batches = np.array_split(X_test, val_batch)\n",
    "    Y_val_batches = np.array_split(Y_test, val_batch)\n",
    "        # Loop over all batches\n",
    "    for i in range(val_batch):\n",
    "        batch_x_val, batch_y_val = X_val_batches[i], Y_val_batches[i].reshape(-1,1)\n",
    "    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "    # see no 'optimizer' here\n",
    "        a = sess.run([accuracy], feed_dict={inputs: batch_x_val, targets: batch_y_val})\n",
    "\n",
    "            #----------------------------------------------------------\n",
    "            # in Keras\n",
    "            # model.fit(x_train,y_train,epochs=5, batch_size=32)\n",
    "            #-----------------------------------------------------------\n",
    "            # Compute average loss\n",
    "        val_acc.append(a)\n",
    "    \n",
    "    print(\"Test accuracy is {:.4f}%\".format(np.mean(val_acc)*100))\n",
    "    \n",
    "    # predictions\n",
    "    \n",
    "    # Testing on the unlabelled data\n",
    "    predictions_unlabeled = []\n",
    "    ii = 0\n",
    "    while ii + batch_size <= len(X_unlabeled):\n",
    "    if ii + batch_size > len(X_unlabeled):\n",
    "        batch_size = len(X_unlabeled) - ii\n",
    "    X_batch = X_unlabeled[ii:ii+batch_size]\n",
    "\n",
    "    pred = session.run([prediction], feed_dict={inputs:X_batch})\n",
    "    \n",
    "    predictions_unlabeled.append(pred)\n",
    "    ii += batch_size\n",
    "    \n",
    "    print('Predictions are complete !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_real = []\n",
    "for i in range(len(predictions_unlabeled)):\n",
    "    for ii in range(len(predictions_unlabeled[i][0])):\n",
    "        if predictions_unlabeled[i][0][ii][0] >= 0.5:\n",
    "            pred_real.append(1)\n",
    "        else:\n",
    "            pred_real.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_real[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27512</th>\n",
       "      <td>Quality cheese and uber-sexy Angelina Jolie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23152</th>\n",
       "      <td>Paris Hilton and Sexy Cop.......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26437</th>\n",
       "      <td>And as stupid as San Francisco's road system i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14391</th>\n",
       "      <td>You are a fucking bitch and I think I may hate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4722</th>\n",
       "      <td>ps i LOVE toyota and yeh you need a HIS AND HE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9448</th>\n",
       "      <td>in simple words, in seattle Bank of America is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18451</th>\n",
       "      <td>stupid lakers should have beat suns.....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20140</th>\n",
       "      <td>I like being at Purdue, not with my family.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7672</th>\n",
       "      <td>On the upside, apparently Toyota Siennas are g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11229</th>\n",
       "      <td>I need some of that geico balboa stuff..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>london sucks....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20225</th>\n",
       "      <td>hmm i love san francisco.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13291</th>\n",
       "      <td>i love my new Macbook..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9108</th>\n",
       "      <td>miss you aaa ~..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14158</th>\n",
       "      <td>Three days at Purdue with three awesome people.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963</th>\n",
       "      <td>You are a fucking bitch and I think I may hate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>i want sony vaio!.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24838</th>\n",
       "      <td>the stupid honda lol or a BUG!..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>Oh, how I onced loved to make Harvard undergra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23401</th>\n",
       "      <td>I love Southwest Airlines.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15744</th>\n",
       "      <td>the toyota tercel'96 is doing good!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19349</th>\n",
       "      <td>My Purdue Cal friends are awesome!..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25377</th>\n",
       "      <td>angelina jolie is so beautiful that i don't ev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>It was really ironic that he spent the first p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24604</th>\n",
       "      <td>we need at least ONE beautiful girl at ucla.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4954</th>\n",
       "      <td>i love my honda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12588</th>\n",
       "      <td>Anyway, some crappy Honda, but, as is young pe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21652</th>\n",
       "      <td>I liked Tom Cruise until he dumped Nicole Kidman.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27170</th>\n",
       "      <td>I love paris hilton, britney spears and jessic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6380</th>\n",
       "      <td>We all know that we love Paris Hilton...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21391</th>\n",
       "      <td>I figured San Francisco would be chock full of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27664</th>\n",
       "      <td>THIS SONG SUCKSSSSSSS i hate paris hilton.....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28581</th>\n",
       "      <td>actually the only thing that i really like abo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>UCLA was stupid.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>I love MIT so much...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23131</th>\n",
       "      <td>stupid lakers....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23672</th>\n",
       "      <td>And I would love myself a Macbook ( to be used...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7685</th>\n",
       "      <td>well i'm gonna go enjoy being in seattle.....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15244</th>\n",
       "      <td>we need at least ONE beautiful girl at ucla.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10286</th>\n",
       "      <td>I told her that UCLA is excellent for both...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24874</th>\n",
       "      <td>I LOVE UCLA.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14622</th>\n",
       "      <td>I've been working on an article, and Antid Oto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>mostly these stupid guys with their shitty hon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15155</th>\n",
       "      <td>Though I want a Vios / Honda Jazz badly, I thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11017</th>\n",
       "      <td>And Tom Cruise is beautiful....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19506</th>\n",
       "      <td>I love London.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12265</th>\n",
       "      <td>gawssh i hate london i hope he blows up and hi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25429</th>\n",
       "      <td>I hate Citi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24587</th>\n",
       "      <td>The seattle scene is pretty amazing too....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28697</th>\n",
       "      <td>�  �  And her �  charming prince �  in �  a sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11807</th>\n",
       "      <td>Love my Lakers..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17110</th>\n",
       "      <td>I like Boston – so historic and lots to do – e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10308</th>\n",
       "      <td>I love paris hilton...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25697</th>\n",
       "      <td>I love my San Francisco Nights, and I feel gyp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28147</th>\n",
       "      <td>Seattle sucks in bumper to bumper if you're on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16091</th>\n",
       "      <td>Since then, 25 automakers including Toyota Mot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9228</th>\n",
       "      <td>I LOVE the Geico lizard.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16418</th>\n",
       "      <td>Awesome job AAA for putting on an awesome show!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9265</th>\n",
       "      <td>This superb View London Christmas gift basket ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19207</th>\n",
       "      <td>The citizens and government of San Francisco a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review  Classes\n",
       "27512     Quality cheese and uber-sexy Angelina Jolie...        1\n",
       "23152                   Paris Hilton and Sexy Cop.......        1\n",
       "26437  And as stupid as San Francisco's road system i...        1\n",
       "14391  You are a fucking bitch and I think I may hate...        1\n",
       "4722   ps i LOVE toyota and yeh you need a HIS AND HE...        1\n",
       "9448   in simple words, in seattle Bank of America is...        1\n",
       "18451           stupid lakers should have beat suns.....        1\n",
       "20140        I like being at Purdue, not with my family.        1\n",
       "7672   On the upside, apparently Toyota Siennas are g...        1\n",
       "11229           I need some of that geico balboa stuff..        1\n",
       "3254                                    london sucks....        1\n",
       "20225                          hmm i love san francisco.        1\n",
       "13291                            i love my new Macbook..        1\n",
       "9108                                    miss you aaa ~..        1\n",
       "14158    Three days at Purdue with three awesome people.        1\n",
       "3963   You are a fucking bitch and I think I may hate...        1\n",
       "6098                                  i want sony vaio!.        1\n",
       "24838                   the stupid honda lol or a BUG!..        1\n",
       "3463   Oh, how I onced loved to make Harvard undergra...        1\n",
       "23401                         I love Southwest Airlines.        1\n",
       "15744                the toyota tercel'96 is doing good!        1\n",
       "19349               My Purdue Cal friends are awesome!..        1\n",
       "25377  angelina jolie is so beautiful that i don't ev...        1\n",
       "1153   It was really ironic that he spent the first p...        1\n",
       "24604       we need at least ONE beautiful girl at ucla.        1\n",
       "4954                                  i love my honda...        1\n",
       "12588  Anyway, some crappy Honda, but, as is young pe...        1\n",
       "21652  I liked Tom Cruise until he dumped Nicole Kidman.        1\n",
       "27170  I love paris hilton, britney spears and jessic...        1\n",
       "6380            We all know that we love Paris Hilton...        1\n",
       "...                                                  ...      ...\n",
       "21391  I figured San Francisco would be chock full of...        1\n",
       "27664     THIS SONG SUCKSSSSSSS i hate paris hilton.....        1\n",
       "28581  actually the only thing that i really like abo...        1\n",
       "4454                                    UCLA was stupid.        1\n",
       "145                                I love MIT so much...        1\n",
       "23131                                  stupid lakers....        1\n",
       "23672  And I would love myself a Macbook ( to be used...        1\n",
       "7685       well i'm gonna go enjoy being in seattle.....        1\n",
       "15244       we need at least ONE beautiful girl at ucla.        1\n",
       "10286      I told her that UCLA is excellent for both...        1\n",
       "24874                                       I LOVE UCLA.        1\n",
       "14622  I've been working on an article, and Antid Oto...        1\n",
       "24995  mostly these stupid guys with their shitty hon...        1\n",
       "15155  Though I want a Vios / Honda Jazz badly, I thi...        1\n",
       "11017                    And Tom Cruise is beautiful....        1\n",
       "19506                                     I love London.        1\n",
       "12265  gawssh i hate london i hope he blows up and hi...        1\n",
       "25429                                     I hate Citi...        1\n",
       "24587        The seattle scene is pretty amazing too....        1\n",
       "28697  �  �  And her �  charming prince �  in �  a sh...        1\n",
       "11807                                   Love my Lakers..        1\n",
       "17110  I like Boston – so historic and lots to do – e...        1\n",
       "10308                             I love paris hilton...        1\n",
       "25697  I love my San Francisco Nights, and I feel gyp...        1\n",
       "28147  Seattle sucks in bumper to bumper if you're on...        1\n",
       "16091  Since then, 25 automakers including Toyota Mot...        1\n",
       "9228                            I LOVE the Geico lizard.        1\n",
       "16418   Awesome job AAA for putting on an awesome show!!        1\n",
       "9265   This superb View London Christmas gift basket ...        1\n",
       "19207  The citizens and government of San Francisco a...        1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.savetxt('predictions.txt', pred_real)\n",
    "new_dataframe = test_data[:len(pred_real)]\n",
    "new_dataframe['Classes'] = pred_real\n",
    "new_dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
