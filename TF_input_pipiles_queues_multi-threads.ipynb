{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We simulate some raw input data \n",
    "# (think about it as fetching some data from the file system, say using feed_dict{})\n",
    "# let's say: batches of 128 samples, each containing 1024 data points \n",
    "x_input_data = tf.random_normal([128, 1024], mean=0, stddev=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the queue and deque part\n",
    "\n",
    "# We build a FIFOQueue inside the graph \n",
    "# 1. an object 'q' is created which will wait for the data (a batch of 5 here)\n",
    "#    'q' is empty until we run a session and feed the data to it\n",
    "\n",
    "# 2. We need an operation that will actually fill the queue with our data\n",
    "#    q.enqueue fetches the data into 'q'.\n",
    "#    \"enqueue_many\" slices \"x_input_data\" along the 0th dimension to make multiple queue elements\n",
    "\n",
    "# 3. To leverage multi-threading we create a \"QueueRunner\"\n",
    "#    that will handle the \"enqueue_op\" outside of the main thread\n",
    "#    We don't need much parallelism here, so we will use only 1 thread\n",
    "#    (without step 3, its simply running on one main thread)\n",
    "# 4. We need a dequeue op to get the next elements in the queue following the FIFO policy.\n",
    "\n",
    "with tf.variable_scope(\"queue\"):\n",
    "    q = tf.FIFOQueue(capacity=5, dtypes=tf.float32) # enqueue 5 batches\n",
    "    # We use the \"enqueue\" operation so 1 element of the queue is the full batch\n",
    "    enqueue_op = q.enqueue(x_input_data) # we are loading 128 records as 1 batch\n",
    "    \n",
    "    # To leverage multi-threading we create a \"QueueRunner\"\n",
    "    # that will handle the \"enqueue_op\" outside of the main thread\n",
    "    # We don't need much parallelism here, so we will use only 1 thread\n",
    "    numberOfThreads = 1\n",
    "    qr = tf.train.QueueRunner(q, [enqueue_op] * numberOfThreads)\n",
    "    #  must need to add \"QueueRunner\" to the QUEUE_RUNNERS collection\n",
    "    tf.train.add_queue_runner(qr)\n",
    "    \n",
    "    input = q.dequeue() # It replaces our input placeholder\n",
    "    # The input tensor is the equivalent of a placeholder now \n",
    "    # but directly connected to the data sources in the graph\n",
    "\n",
    "    # We can also compute y_true right into the graph now\n",
    "    y_true = tf.cast(tf.reduce_sum(input, axis=1, keep_dims=True) > 0, tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We build our small model: a basic two layers neural net with ReLU\n",
    "\n",
    "with tf.variable_scope('FullyConnected'):\n",
    "    w = tf.get_variable('w', shape=[1024, 1024], initializer=tf.random_normal_initializer(stddev=1e-1))\n",
    "    b = tf.get_variable('b', shape=[1024], initializer=tf.constant_initializer(0.1))\n",
    "    z = tf.matmul(input, w) + b\n",
    "    y = tf.nn.relu(z)\n",
    "\n",
    "    w2 = tf.get_variable('w2', shape=[1024, 1], initializer=tf.random_normal_initializer(stddev=1e-1))\n",
    "    b2 = tf.get_variable('b2', shape=[1], initializer=tf.constant_initializer(0.1))\n",
    "    z = tf.matmul(y, w2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss, accuracy and optimizer\n",
    "\n",
    "with tf.variable_scope('Loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(None, tf.cast(y_true, tf.float32), z)\n",
    "    loss_op = tf.reduce_mean(losses)\n",
    "\n",
    "with tf.variable_scope('Accuracy'):\n",
    "    y_pred = tf.cast(z > 0, tf.int32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
    "    accuracy = tf.Print(accuracy, data=[accuracy], message=\"accuracy:\")\n",
    "\n",
    "# We add the training op ...\n",
    "adam = tf.train.AdamOptimizer(1e-2)\n",
    "train_op = adam.minimize(loss_op, name=\"train_op\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "with tf.Session() as sess:\n",
    "    # ... init our variables, ...\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # ... add the coordinator, ...\n",
    "    # build the coordinator to coordinate our child threads with the main thread\n",
    "    coord = tf.train.Coordinator()\n",
    "    # you need to start all your queues before runnig anything otherwise,\n",
    "    # The main threads will wait for them to start and you will hang again\n",
    "    # This helper start all queues in tf.GraphKeys.QUEUE_RUNNERS\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    # The QueueRunner will automatically call the enqueue operation\n",
    "    # asynchronously in its own thread ensuring that the queue is always full\n",
    "    # No more hanging for the main process\n",
    "    \n",
    "    # ... check the accuracy before training (without feed_dict!), ...\n",
    "    sess.run(accuracy)\n",
    "\n",
    "    # ... train ...\n",
    "    for i in range(5000):\n",
    "        #  ... without sampling from Python (no batch creation etc.) and without a feed_dict !\n",
    "        _, loss = sess.run([train_op, loss_op])\n",
    "\n",
    "        # We regularly check the loss\n",
    "        if i % 500 == 0:\n",
    "            print('iter:%d - loss:%f' % (i, loss))\n",
    "\n",
    "    # Finally, we check our final accuracy\n",
    "    sess.run(accuracy)\n",
    "    \n",
    "    # stop the child threads \n",
    "    coord.request_stop()\n",
    "    \n",
    "    # wait for the child threads to stop before releasing the main thread\n",
    "    coord.join(threads)\n",
    "\n",
    "print(\"Time taken: %f\" % (time.time() - startTime))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
